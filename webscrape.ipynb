{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# webscrape.ipynb\n",
    "## pre-postprocessing, pre-model-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_match(data, link) -> str:\n",
    "    if len(link.attrs['href']) == 0: return ''\n",
    "    match = re.search(DIRECT_SUB_DOMAINS, link.attrs['href'])\n",
    "    new_target = \"\"\n",
    "    sep = ''\n",
    "    if match:\n",
    "        new_target = link.attrs['href']\n",
    "    elif \".\" not in link.attrs['href'] and is_entrypoint(link.attrs['href']):\n",
    "        print(\"RELATIVE MATCH (SITEMAP): \", link.attrs['href'])\n",
    "        if link.attrs['href'][0] != '/': sep = '/'\n",
    "        new_target = data[DOMAIN] + sep + link.attrs['href']\n",
    "    elif \".\" not in link.attrs['href'] and is_entrypoint(data[DOMAIN]):\n",
    "        print(\"RELATIVE MATCH (INTRA-SITEMAP): \", link.attrs['href'])\n",
    "        base_match = re.search(\"^([^\\/]+)\\/\", data[DOMAIN])\n",
    "        if base_match != None:\n",
    "            base = base_match.group(1)\n",
    "        if link.attrs['href'][0] != '/': sep = '/'\n",
    "        new_target = base + sep + link.attrs['href']\n",
    "    elif \".\" not in link.attrs['href']:\n",
    "        print(\"RELATIVE MATCH (NON-SITEMAP): \", link.attrs['href'])\n",
    "        if link.attrs['href'][0] != '/': sep = '/'\n",
    "        new_target = data[DOMAIN] + sep + link.attrs['href']\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_about_pages(links, data):\n",
    "    href_list = []\n",
    "    filtered_links = []\n",
    "    for link in links: \n",
    "        if 'href' in link.attrs.keys():\n",
    "            char_set = set(link.attrs['href'])\n",
    "            if char_set.isdisjoint({'#','(',')','@'}) and '.pdf' not in link.attrs['href']:\n",
    "                href_list.append(link.attrs['href'])\n",
    "                filtered_links.append(link)\n",
    "    entry = {\n",
    "        'id': data[ID],\n",
    "        'origin': complete_url(data[DOMAIN]),\n",
    "        'all_link_count': len(links),\n",
    "        'links': str(href_list),\n",
    "    }\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    client.data.sublinks.insert_one(entry)\n",
    "    client.close()\n",
    "\n",
    "    selected_urls = None\n",
    "    if len(filtered_links) > MAX_SUBPAGES:\n",
    "        selected_urls = []\n",
    "        indices = np.random.choice(len(filtered_links), MAX_SUBPAGES, replace=False)\n",
    "        for index in indices:\n",
    "            res = url_match(data, filtered_links[index])\n",
    "            if res != \"\" and re.search(DIRECT_SUB_DOMAINS, complete_url(res)):\n",
    "                selected_urls.append(filtered_links[index])\n",
    "    else: selected_urls = filtered_links\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(fetch_html, data, url_match(data, link)): \\\n",
    "                link for link in selected_urls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(tags):\n",
    "    texts = []\n",
    "    for t in tags:\n",
    "        inner_match = re.search(\"<.*>([\\w -,.!?\\\"\\']+)<.*>\", str(t))\n",
    "        if inner_match:\n",
    "            text = inner_match.group(1)\n",
    "            texts.append(text.replace(\"\\\"\", '\\''))\n",
    "    text = ' '.join(texts)\n",
    "    if len(text) < LEN_CUTOFF: return None\n",
    "    else: return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_url(base_url) -> str:\n",
    "    if 'https://www.' not in base_url: \n",
    "        return 'https://www.' + base_url\n",
    "    else: return base_url\n",
    "\n",
    "def is_entrypoint(href) -> bool:\n",
    "    for elem in ENTRYPOINT_POSITIVES:\n",
    "        if elem in href:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_site_map(links, data) -> list:\n",
    "    site_map_targets = []\n",
    "    for link in links:\n",
    "        if 'href' in link.attrs.keys() and is_entrypoint(link.attrs['href']):\n",
    "            new_target = url_match(data, link)\n",
    "            if new_target != \"\":\n",
    "                site_map_targets.append(new_target)\n",
    "    return site_map_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(data, base_url):\n",
    "    error = ''\n",
    "    if base_url == '': return None\n",
    "    full_url = complete_url(base_url)\n",
    "    try:\n",
    "        result = requests.get(full_url, timeout=TIMEOUT, headers=np.random.choice(HEADERS))\n",
    "        status = result.status_code\n",
    "        print(\"URL: \", full_url)\n",
    "        print(\"STATUS: \", status)\n",
    "        src = result.content\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "        html_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        text = extract_text(html_tags)\n",
    "\n",
    "        if result.status_code != 200: text = u''\n",
    "        elif result.status_code == 200:\n",
    "            document = {\n",
    "                'id': data[ID],\n",
    "                'endpoint': full_url,\n",
    "                'status': result.status_code,\n",
    "                'time': datetime.datetime.now(),\n",
    "                'text': text\n",
    "            }\n",
    "            client = MongoClient(DB_CONNECTION)\n",
    "            if not client.data.companies.find_one({'id': data[ID]}):\n",
    "                company = {\n",
    "                    'id': data[ID],\n",
    "                    'name': data[NAME],\n",
    "                    'domain': data[DOMAIN],\n",
    "                    'year_founded': data[YEAR_FOUNDED],\n",
    "                    'industry': data[INDUSTRY],\n",
    "                    'size_range': data[SIZE_RANGE],\n",
    "                    'locality': data[LOCALITY],\n",
    "                    'country': data[COUNTRY],\n",
    "                    'linked_in_url': data[LINKEDIN_URL],\n",
    "                    'relevant': data[-1]\n",
    "                }\n",
    "                client.data.companies.insert_one(company)\n",
    "            client.data.documents.insert_one(document)\n",
    "            client.close()\n",
    "        return soup\n",
    "    except requests.exceptions.Timeout:\n",
    "        error = 'timeout'\n",
    "    except requests.exceptions.SSLError:\n",
    "        error = 'too many retries'\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        error = 'too many redirects'\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        error = 'refused connection'\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    failure = {\n",
    "        'id': data[ID],\n",
    "        'endpoint': full_url,\n",
    "        'time': datetime.datetime.now(),\n",
    "        'error': error\n",
    "    }\n",
    "    client.data.failures.insert_one(failure)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_unit(data, is_site_map):\n",
    "    if is_site_map:\n",
    "        soup = fetch_html(data, data[DOMAIN])\n",
    "        if soup != None:\n",
    "            links = soup.find_all(\"a\")\n",
    "            find_about_pages(links, data)\n",
    "    else:\n",
    "        soup = fetch_html(data, data[DOMAIN])\n",
    "        if soup != None:\n",
    "            links = soup.find_all(\"a\")\n",
    "            site_map_targets = find_site_map(links, data)\n",
    "            if len(site_map_targets) == 0:\n",
    "                find_about_pages(links, data)\n",
    "            else:\n",
    "                print(site_map_targets)\n",
    "                for target in site_map_targets:\n",
    "                    data[DOMAIN] = target\n",
    "                    work_unit(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_work(data):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(work_unit, obj, False): \\\n",
    "                obj for obj in data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sam_entities_data(num_enqueues, src_file):\n",
    "    data = []\n",
    "    counter = 0\n",
    "    with open(src_file) as jsonfile:\n",
    "            entities = json.load(jsonfile)\n",
    "            for entry in entities['domain_agent']:\n",
    "                if re.search(TARGET_URLS, entry['domain_agent_url']):\n",
    "                    counter += 1\n",
    "                    if str(counter) not in PREVIOUS_SCRAPES:\n",
    "                        row = [str(counter), entry['domain_agent_name'], \\\n",
    "                            entry['domain_agent_url'], '', entry['attribute_agent'], \\\n",
    "                                '', '', '', '', '', '']\n",
    "                        data.append(row)\n",
    "                    if counter > num_enqueues: break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE STRICTLY W/ PEOPLE DATA LABS DATASET\n",
    "def enqueue_work_units(is_binary, class_rerouting, src_file):\n",
    "    counter = 0\n",
    "    class_data = defaultdict(list)\n",
    "    with open(src_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "        for row in reader:\n",
    "            if counter == 0:\n",
    "                counter += 1 \n",
    "                continue # skip csv header row\n",
    "            current_url = row[DOMAIN]\n",
    "            if re.search(TARGET_URLS, current_url) and \\\n",
    "                    row[ID] not in PREVIOUS_SCRAPES and \\\n",
    "                    int(row[EMPLOYEE_ESTIMATE]) > MIN_EMPLOYEES:\n",
    "                counter += 1\n",
    "                custom_row = row[0:4]\n",
    "                # UNCOMMENT BELOW IF MULTICLASS\n",
    "                #custom_row.append(class_rerouting[row[INDUSTRY]]) \n",
    "                custom_row.append(row[INDUSTRY])\n",
    "                custom_row.extend(row[5:])\n",
    "                # BINARIZE LABELS - row size is larger in binary mode\n",
    "                if is_binary:\n",
    "                    label = ''\n",
    "                    if row[INDUSTRY] in RELEVANT_CLASSES:\n",
    "                        label = '1'\n",
    "                        custom_row.append(1)\n",
    "                    else:\n",
    "                        label = '0'\n",
    "                        custom_row.append(0)\n",
    "                    class_data[label].append(custom_row)\n",
    "                if counter > 200000: break\n",
    "    return class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_sample_classes(class_data):\n",
    "    data = []\n",
    "    for key,_ in class_data.items():\n",
    "        indices = np.random.choice(len(class_data[key]), CLASS_BALANCE_THRESH, replace=False)\n",
    "        for index in indices:\n",
    "            data.append(class_data[key][index])\n",
    "    np.random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enqueue_training_data(src_file):\n",
    "    class_data = defaultdict(list)\n",
    "    with open(src_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "        for row in reader:\n",
    "            if row[0] == 'class': continue\n",
    "            reformatted_row = row[1:]\n",
    "            reformatted_row.append(row[0])\n",
    "            class_data[row[0]].append(reformatted_row)\n",
    "    return class_data\n",
    "\n",
    "def write_training_metadata(data, filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        for sample in data:\n",
    "            new = [sample[-1]]+sample[:-2]\n",
    "            write = csv.writer(f)\n",
    "            write.writerow(new)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_classes(src_file):\n",
    "    relevant_classes = set()\n",
    "    with open(src_file) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\n')\n",
    "        for r in reader:\n",
    "            relevant_classes.add(r[0].split(': ')[0])\n",
    "    return relevant_classes\n",
    "\n",
    "def populate_class_mapping(src_file):\n",
    "    custom_class_map = {}\n",
    "    with open(src_file) as csvfile:\n",
    "        reader = csv.reader(csvfile,delimiter=',',quotechar='\\\"')\n",
    "        for row in reader:\n",
    "            index = row.index('*')\n",
    "            for i in range(0, index-1):\n",
    "                custom_class_map[row[i]] = row[index-1]\n",
    "    return custom_class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscrape(is_train):\n",
    "    data = None\n",
    "    if is_train:\n",
    "        rel_classes = load_relevant_classes(src_file='./manifests/pointed.csv')\n",
    "        custom_class_map = populate_class_mapping(src_file='./maniefests/collapsed.csv')\n",
    "        class_data = enqueue_work_units(\n",
    "            is_binary=True,\n",
    "            class_rerouting=custom_class_map,\n",
    "            src_file='/inputs/companies_sorted.csv' # ENQUEUE ENTIRE PEOPLE DATA LABS DATASET (PDL)\n",
    "        )\n",
    "        data = randomly_sample_classes(class_data)\n",
    "    else:\n",
    "        data = load_sam_entities_data(\n",
    "            num_enqueues=100,\n",
    "            src_file='./inputs/sam_entities.json' # SAM.GOV ENTITIES JSON DUMP\n",
    "        )\n",
    "    if data: execute_work(data)\n",
    "\n",
    "    \"\"\"\n",
    "    class_data = enqueue_training_data(\n",
    "        src_file='inputs/company_samples_10000_hq.csv' # ENQUEUE PDL SUBSET FROM METADATA\n",
    "    )\n",
    "    write_training_metadata(data, './company_samples_XXXX_Cq.csv') # SAVE PDL COMPANY METADATA FOR REPLICATION PURPOSES\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscrape(is_train=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f497f4381756dce953eb84e5b818cc9658f5d865a3be7b3cf618106cb9f31b3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('kyle-ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
