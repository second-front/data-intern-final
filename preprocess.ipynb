{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess.ipynb\n",
    "### post-webscrape, pre-model-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running running this notebook, database 'data' should contain the following collections from the webscraping process:  \n",
    "```\n",
    "> use data  \n",
    "switched to db data  \n",
    "> show collections  \n",
    "companies  \n",
    "documents  \n",
    "failures  \n",
    "sublinks  \n",
    "```\n",
    "If old collections exist:\n",
    "```  \n",
    "> db.[OLD_COLLECTION].drop()  \n",
    "true\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker pull mongo  \n",
    "docker run -d -v [PWD]:/home \\ \n",
    "-p 27017-27019:27017-27019 \\\n",
    "--name mongodb mongo:latest \\ \n",
    "docker exec -it mongodb bash\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Webscrape MongoDB Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd [DUMP_DIR]  \n",
    "mongorestore -d data ./data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Indexes to Improve Query Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "db.companies.createIndex({\"id\":1})\n",
    "db.documents.createIndex({\"id\":1})\n",
    "db.sublinks.createIndex({\"id\":1}) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "> db.companies.createIndex({\"id\":1})  \n",
    "{  \n",
    "        \"numIndexesBefore\" : 1,  \n",
    "        \"numIndexesAfter\" : 2,  \n",
    "        \"createdCollectionAutomatically\" : false,  \n",
    "        \"ok\" : 1  \n",
    "}  \n",
    "> db.documents.createIndex({\"id\":1})  \n",
    "{  \n",
    "        \"numIndexesBefore\" : 1,  \n",
    "        \"numIndexesAfter\" : 2,  \n",
    "        \"createdCollectionAutomatically\" : false,  \n",
    "        \"ok\" : 1  \n",
    "}  \n",
    "> db.sublinks.createIndex({\"id\":1})  \n",
    "{  \n",
    "        \"numIndexesBefore\" : 1,  \n",
    "        \"numIndexesAfter\" : 2,  \n",
    "        \"createdCollectionAutomatically\" : false,  \n",
    "        \"ok\" : 1  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import concurrent.futures\n",
    "import multiprocessing as mp\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONNECTION = 'mongodb://localhost:27017'\n",
    "from headers import HEADERS, TARGET_URLS, DIRECT_SUB_DOMAINS, ENTRYPOINT_POSITIVES, CUSTOM_STOPWORDS\n",
    "from headers import ID, NAME, DOMAIN, YEAR_FOUNDED, INDUSTRY, SIZE_RANGE, LOCALITY, COUNTRY, LINKEDIN_URL, EMPLOYEE_ESTIMATE, INDEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/admin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "negatives = list(stopwords.words())\n",
    "negatives.extend(CUSTOM_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING PARAMS\n",
    "END_TOKEN = '>'\n",
    "START_TOKEN = '<'\n",
    "MIN_DOC_FREQ = 150 # minimum of documents that an extracted feature needs to appear in\n",
    "MIN_FEATURES = 33 # minimum number of non-stop word features in order to qualify as training example\n",
    "N_GRAM_LOWER_LIM = 1 # lower bound for count vectorizer's n-gram range\n",
    "N_GRAM_UPPER_LIM = 2 # upper ..... ... ..... ............ ...... .....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(tokens, metadata):\n",
    "    try:\n",
    "        link_vectorizer = CountVectorizer(ngram_range=(N_GRAM_LOWER_LIM,N_GRAM_UPPER_LIM))\n",
    "        X1 = link_vectorizer.fit_transform([tokens[0]])\n",
    "        link_grams = link_vectorizer.get_feature_names()\n",
    "        doc_vectorizer = CountVectorizer(ngram_range=(N_GRAM_LOWER_LIM,N_GRAM_UPPER_LIM))\n",
    "        X2 = doc_vectorizer.fit_transform([tokens[1]])\n",
    "        doc_grams = doc_vectorizer.get_feature_names()\n",
    "        training_example = {\n",
    "            'id': metadata[1],\n",
    "            'label': metadata[0],\n",
    "            'relevant': metadata[2],\n",
    "            'link_array': X1.toarray().tolist(),\n",
    "            'link_grams': link_grams,\n",
    "            'doc_array': X2.toarray().tolist(),\n",
    "            'doc_grams': doc_grams\n",
    "        }\n",
    "        client = MongoClient(DB_CONNECTION)\n",
    "        if len(doc_grams) + len(link_grams) > MIN_FEATURES:\n",
    "            client.data.train.insert_one(training_example)\n",
    "        else:\n",
    "            print('not enough tags, moving on...')\n",
    "            client.data.companies.delete_one({'id': metadata[1]})\n",
    "            client.data.sublinks.delete_many({'id': metadata[1]})\n",
    "            client.data.documents.delete_many({'id': metadata[1]})\n",
    "        return\n",
    "    except ValueError:\n",
    "        print('stop words only, moving on...')\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    client.data.companies.delete_one({'id':metadata[1]})\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactored cell below for gensim 4.0 update:\\\n",
    "i.e. \n",
    "```python\n",
    "x in model.vw.vocab.keys() # new\n",
    "```\n",
    "```python\n",
    "x in embeddings.key_to_index() # old\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_links):\n",
    "    links = str(raw_links).strip('][')\n",
    "    while '\\'' in links:\n",
    "        links = links.replace('\\'', START_TOKEN + \" \", 1)\n",
    "        links = links.replace('\\'', \" \" + END_TOKEN, 1)\n",
    "    text = \" \".join(links.split(', '))\n",
    "    pattern = re.compile('[^a-zA-z\\s<>]+', re.UNICODE)\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = ' '.join([x for x in nltk.word_tokenize(text) if (x not in negatives) and (x in model.wv.vocab.keys()) and (len(x) > 3 or x == '<' or x == '>')])\n",
    "    text = text.replace(START_TOKEN, '[S]')\n",
    "    text = text.replace(END_TOKEN, '[E]')\n",
    "    text = text.replace('[S]' + ' ' + '[E]', '')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(links_list, documents):\n",
    "    hp_tokens = ''\n",
    "    for links in links_list:\n",
    "        hp_tokens = tokenize(links)\n",
    "    lp_tokens = tokenize(documents)\n",
    "    return (hp_tokens, lp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_unit(links, documents, metadata):\n",
    "    tokens = preprocess(links, documents)\n",
    "    create_training_example(tokens, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(entry, order_id, max_id, client):\n",
    "    #print('loading...', entry['id'])\n",
    "    if order_id % 100 == 0: print(str(int(order_id/max_id*100))+'%')\n",
    "    links = []\n",
    "    documents = []\n",
    "    #client = MongoClient(DB_CONNECTION)\n",
    "    for link_list in client.data.sublinks.find({'id':entry['id']}):\n",
    "        links.append(link_list['links'])\n",
    "    for doc in client.data.documents.find({'id':entry['id']}):\n",
    "        documents.append(doc['text'])\n",
    "    #client.close()\n",
    "\n",
    "    return (links, documents, entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_db():\n",
    "    entry_list = []\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    for entry in client.data.companies.find():\n",
    "        entry_list.append(entry)\n",
    "    #client.close()\n",
    "    metadata = []\n",
    "    links_list = []\n",
    "    documents_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        future_to_load = {\n",
    "            executor.submit(load, obj[1], obj[0], len(entry_list), client): \\\n",
    "                obj for obj in enumerate(entry_list)\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_load):\n",
    "            res = future.result()\n",
    "            links_list.append(res[0])\n",
    "            documents_list.append(str(res[1]))\n",
    "            metadata.append(tuple((res[2]['industry'], res[2]['id'], res[2]['relevant'])))\n",
    "    client.close()\n",
    "    return (links_list, documents_list, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function submit_for_processing() creates the db.train collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_for_processing():\n",
    "    links_list, documents_list, metadata = load_from_db()\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        future_to_preprocess = {\n",
    "            executor.submit(preprocessing_unit, obj[0], obj[1], obj[2]): \\\n",
    "                obj for obj in zip(links_list, documents_list, metadata)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(lemma,w,p):\n",
    "    if p is not None: return lemma.lemmatize(w,p)\n",
    "    else: return lemma.lemmatize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def vocab_work_unit(entry_id, order_id, max_id, lemma, client):\\n    if order_id % 100 == 0: print(str(int(order_id/max_id*100))+\\'%\\')\\n    entry = client.data.train.find_one({\\'id\\':entry_id})\\n    #lemma = WordNetLemmatizer()\\n    doc_res = [n_gram.strip().split() for n_gram in entry[\\'doc_grams\\']]\\n    link_res = [n_gram.strip().split() for n_gram in entry[\\'link_grams\\']]\\n    # LEMMATIZE ACCORDING TO PART-OF-SPEECH ESTIMATE\\n    stemmed_d = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, doc_res))]\\n    stemmed_l = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, link_res))]\\n    combined_grams = stemmed_d + stemmed_l\\n    combined_freqs = []\\n    combined_freqs.extend(entry[\\'doc_array\\'][0])\\n    combined_freqs.extend(entry[\\'link_array\\'][0])\\n    document = {\\n        \\'id\\': entry[\\'id\\'],\\n        \\'label\\': entry[\\'relevant\\'],\\n        \\'combined_freqs\\': combined_freqs,\\n        \\'combined_grams\\': combined_grams\\n    }\\n    #client = MongoClient(DB_CONNECTION)\\n    client.data.norm.stemmed.insert_one(document)\\n    #client.close()\\n    return (combined_grams,entry[\\'id\\'])'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def vocab_work_unit(entry_id, order_id, max_id, lemma, client):\n",
    "    if order_id % 100 == 0: print(str(int(order_id/max_id*100))+'%')\n",
    "    entry = client.data.train.find_one({'id':entry_id})\n",
    "    #lemma = WordNetLemmatizer()\n",
    "    doc_res = [n_gram.strip().split() for n_gram in entry['doc_grams']]\n",
    "    link_res = [n_gram.strip().split() for n_gram in entry['link_grams']]\n",
    "    # LEMMATIZE ACCORDING TO PART-OF-SPEECH ESTIMATE\n",
    "    stemmed_d = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, doc_res))]\n",
    "    stemmed_l = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, link_res))]\n",
    "    combined_grams = stemmed_d + stemmed_l\n",
    "    combined_freqs = []\n",
    "    combined_freqs.extend(entry['doc_array'][0])\n",
    "    combined_freqs.extend(entry['link_array'][0])\n",
    "    document = {\n",
    "        'id': entry['id'],\n",
    "        'label': entry['relevant'],\n",
    "        'combined_freqs': combined_freqs,\n",
    "        'combined_grams': combined_grams\n",
    "    }\n",
    "    #client = MongoClient(DB_CONNECTION)\n",
    "    client.data.norm.stemmed.insert_one(document)\n",
    "    #client.close()\n",
    "    return (combined_grams,entry['id'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def create_vocabulary(out_file):\\n    corpus = set()\\n    lemma = WordNetLemmatizer()\\n    document_freq = defaultdict(int)\\n    client = MongoClient(DB_CONNECTION)\\n    entries = [entry['id'] for entry in client.data.train.find()]\\n    train_set = set(np.random.choice(entries, size=int(len(entries)*0.9), replace=False))\\n    print(len(train_set))\\n    print(len(entries))\\n    #entry_list = []\\n    # CREATE LEMMATIZED TRAINING EXAMPLE & INSERT INTO DB.NORM.STEMMED COLLECTION\\n    #for entry in client.data.train.find(): entry_list.append(entry)\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        future_to_lemma = {\\n            executor.submit(vocab_work_unit, obj[1], obj[0], len(entries), lemma, client):                 obj for obj in enumerate(entries)\\n        }\\n        for future in concurrent.futures.as_completed(future_to_lemma):\\n            res = future.result()\\n            # DO NOT INCLUDE FEATURES EXTRACTED FROM VALIDATION SET EXAMPLES INTO VOCABULARY\\n            # VALIDATION SET NEEDS TO BE 'UNSEEN'\\n            if res[1] in train_set:\\n                corpus.update(res[0])\\n            for elem in res[0]:\\n                document_freq[elem] += 1\\n    # CREATE ENUMERATED VOCABULARY\\n    corpus_subset = {val for val in corpus if document_freq[val] > MIN_DOC_FREQ}\\n    vocabulary = {item:val for val,item in enumerate(corpus_subset)}\\n    # EXPORT VOCABULARY\\n    with open(out_file, 'a') as f:\\n        for key,val in vocabulary.items():\\n            f.write(str(val)+','+str(key)+'\\n')\\n        f.close()\\n    # CREATE DENSE TRAINING EXAMPLE\\n    for entry in client.data.norm.stemmed.find():\\n        features = [(vocabulary[a],b) for a,b in zip(entry['combined_grams'], entry['combined_freqs']) if a in vocabulary.keys()]\\n        dense_example = {\\n            'id': entry['id'],\\n            'label': entry['label'],\\n            'features': features\\n        }\\n        if entry['id'] in train_set:\\n            client.data.norm.train.vectorized.insert_one(dense_example)\\n        else:\\n            client.data.norm.test.vectorized.insert_one(dense_example)\\n    client.close()\\n    print(dense_example)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def create_vocabulary(out_file):\n",
    "    corpus = set()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    document_freq = defaultdict(int)\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    entries = [entry['id'] for entry in client.data.train.find()]\n",
    "    train_set = set(np.random.choice(entries, size=int(len(entries)*0.9), replace=False))\n",
    "    print(len(train_set))\n",
    "    print(len(entries))\n",
    "    #entry_list = []\n",
    "    # CREATE LEMMATIZED TRAINING EXAMPLE & INSERT INTO DB.NORM.STEMMED COLLECTION\n",
    "    #for entry in client.data.train.find(): entry_list.append(entry)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_lemma = {\n",
    "            executor.submit(vocab_work_unit, obj[1], obj[0], len(entries), lemma, client): \\\n",
    "                obj for obj in enumerate(entries)\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_lemma):\n",
    "            res = future.result()\n",
    "            # DO NOT INCLUDE FEATURES EXTRACTED FROM VALIDATION SET EXAMPLES INTO VOCABULARY\n",
    "            # VALIDATION SET NEEDS TO BE 'UNSEEN'\n",
    "            if res[1] in train_set:\n",
    "                corpus.update(res[0])\n",
    "            for elem in res[0]:\n",
    "                document_freq[elem] += 1\n",
    "    # CREATE ENUMERATED VOCABULARY\n",
    "    corpus_subset = {val for val in corpus if document_freq[val] > MIN_DOC_FREQ}\n",
    "    vocabulary = {item:val for val,item in enumerate(corpus_subset)}\n",
    "    # EXPORT VOCABULARY\n",
    "    with open(out_file, 'a') as f:\n",
    "        for key,val in vocabulary.items():\n",
    "            f.write(str(val)+','+str(key)+'\\n')\n",
    "        f.close()\n",
    "    # CREATE DENSE TRAINING EXAMPLE\n",
    "    for entry in client.data.norm.stemmed.find():\n",
    "        features = [(vocabulary[a],b) for a,b in zip(entry['combined_grams'], entry['combined_freqs']) if a in vocabulary.keys()]\n",
    "        dense_example = {\n",
    "            'id': entry['id'],\n",
    "            'label': entry['label'],\n",
    "            'features': features\n",
    "        }\n",
    "        if entry['id'] in train_set:\n",
    "            client.data.norm.train.vectorized.insert_one(dense_example)\n",
    "        else:\n",
    "            client.data.norm.test.vectorized.insert_one(dense_example)\n",
    "    client.close()\n",
    "    print(dense_example)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(out_file):\n",
    "    corpus = set()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    document_freq = defaultdict(int)\n",
    "    client = MongoClient(DB_CONNECTION)\n",
    "    entries = [entry['id'] for entry in client.data.train.find()]\n",
    "    train_set = set(np.random.choice(entries, size=int(len(entries)*0.9), replace=False))\n",
    "    print(len(train_set))\n",
    "    max_id = len(entries)\n",
    "    print(max_id)\n",
    "    # CREATE LEMMATIZED TRAINING EXAMPLES & INSERT INTO DB.NORM.STEMMED COLLECTION\n",
    "    for order_id, entry in enumerate(client.data.train.find()):\n",
    "        if order_id % 100 == 0: print(str(int(order_id/max_id*100))+'%')\n",
    "        entry = client.data.train.find_one({'id':entry['id']})\n",
    "        doc_res = [n_gram.strip().split() for n_gram in entry['doc_grams']]\n",
    "        link_res = [n_gram.strip().split() for n_gram in entry['link_grams']]\n",
    "        # LEMMATIZE ACCORDING TO PART-OF-SPEECH ESTIMATE\n",
    "        stemmed_d = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, doc_res))]\n",
    "        stemmed_l = [\" \".join([stem(lemma,w,pos_tagger(p)) for w,p in elem]) for elem in list(map(nltk.pos_tag, link_res))]\n",
    "        combined_grams = stemmed_d + stemmed_l\n",
    "        combined_freqs = []\n",
    "        combined_freqs.extend(entry['doc_array'][0])\n",
    "        combined_freqs.extend(entry['link_array'][0])\n",
    "        document = {\n",
    "            'id': entry['id'],\n",
    "            'label': entry['relevant'],\n",
    "            'combined_freqs': combined_freqs,\n",
    "            'combined_grams': combined_grams\n",
    "        }\n",
    "        client.data.norm.stemmed2.insert_one(document)\n",
    "        # DO NOT INCLUDE FEATURES EXTRACTED FROM VALIDATION SET EXAMPLES INTO VOCABULARY\n",
    "        # VALIDATION SET NEEDS TO BE 'UNSEEN'\n",
    "        if entry['id'] in train_set:\n",
    "            corpus.update(combined_grams)\n",
    "        for elem in combined_grams:\n",
    "            document_freq[elem] += 1\n",
    "    # CREATE ENUMERATED VOCABULARY\n",
    "    corpus_subset = {val for val in corpus if document_freq[val] > MIN_DOC_FREQ}\n",
    "    vocabulary = {item:val for val,item in enumerate(corpus_subset)}\n",
    "    # EXPORT VOCABULARY\n",
    "    with open(out_file, 'a') as f:\n",
    "        for key,val in vocabulary.items():\n",
    "            f.write(str(val)+','+str(key)+'\\n')\n",
    "        f.close()\n",
    "    # CREATE DENSE TRAINING EXAMPLE\n",
    "    for entry in client.data.norm.stemmed2.find():\n",
    "        features = [(vocabulary[a],b) for a,b in zip(entry['combined_grams'], entry['combined_freqs']) if a in vocabulary.keys()]\n",
    "        dense_example = {\n",
    "            'id': entry['id'],\n",
    "            'label': entry['label'],\n",
    "            'features': features\n",
    "        }\n",
    "        if entry['id'] in train_set:\n",
    "            client.data.norm.train2.vectorized.insert_one(dense_example)\n",
    "        else:\n",
    "            client.data.norm.test2.vectorized.insert_one(dense_example)\n",
    "    client.close()\n",
    "    print(dense_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running below cell, database 'data' should contain the following collections:\n",
    "```\n",
    "> show collections\n",
    "companies\n",
    "documents\n",
    "failures\n",
    "norm.stemmed # new\n",
    "norm.test.vectorized # new\n",
    "norm.train.vectorized # new\n",
    "sublinks\n",
    "train # new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit_for_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create db.train index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "> db.train.createIndex({\"id\":1})  \n",
    "{  \n",
    "        \"numIndexesBefore\" : 1,  \n",
    "        \"numIndexesAfter\" : 2,  \n",
    "        \"createdCollectionAutomatically\" : false,  \n",
    "        \"ok\" : 1  \n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51384\n",
      "57103\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "1%\n",
      "1%\n",
      "1%\n",
      "1%\n",
      "1%\n",
      "1%\n",
      "2%\n",
      "2%\n",
      "2%\n",
      "2%\n",
      "2%\n",
      "2%\n",
      "3%\n",
      "3%\n",
      "3%\n",
      "3%\n",
      "3%\n",
      "4%\n",
      "4%\n",
      "4%\n",
      "4%\n",
      "4%\n",
      "4%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "6%\n",
      "6%\n",
      "6%\n",
      "6%\n",
      "6%\n",
      "7%\n",
      "7%\n",
      "7%\n",
      "7%\n",
      "7%\n",
      "7%\n",
      "8%\n",
      "8%\n",
      "8%\n",
      "8%\n",
      "8%\n",
      "8%\n",
      "9%\n",
      "9%\n",
      "9%\n",
      "9%\n",
      "9%\n",
      "9%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "11%\n",
      "11%\n",
      "11%\n",
      "11%\n",
      "11%\n",
      "11%\n",
      "12%\n",
      "12%\n",
      "12%\n",
      "12%\n",
      "12%\n",
      "12%\n",
      "13%\n",
      "13%\n",
      "13%\n",
      "13%\n",
      "13%\n",
      "14%\n",
      "14%\n",
      "14%\n",
      "14%\n",
      "14%\n",
      "14%\n",
      "15%\n",
      "15%\n",
      "15%\n",
      "15%\n",
      "15%\n",
      "15%\n",
      "16%\n",
      "16%\n",
      "16%\n",
      "16%\n",
      "16%\n",
      "16%\n",
      "17%\n",
      "17%\n",
      "17%\n",
      "17%\n",
      "17%\n",
      "18%\n",
      "18%\n",
      "18%\n",
      "18%\n",
      "18%\n",
      "18%\n",
      "19%\n",
      "19%\n",
      "19%\n",
      "19%\n",
      "19%\n",
      "19%\n",
      "20%\n",
      "20%\n",
      "20%\n",
      "20%\n",
      "20%\n",
      "21%\n",
      "21%\n",
      "21%\n",
      "21%\n",
      "21%\n",
      "21%\n",
      "22%\n",
      "22%\n",
      "22%\n",
      "22%\n",
      "22%\n",
      "22%\n",
      "23%\n",
      "23%\n",
      "23%\n",
      "23%\n",
      "23%\n",
      "23%\n",
      "24%\n",
      "24%\n",
      "24%\n",
      "24%\n",
      "24%\n",
      "25%\n",
      "25%\n",
      "25%\n",
      "25%\n",
      "25%\n",
      "25%\n",
      "26%\n",
      "26%\n",
      "26%\n",
      "26%\n",
      "26%\n",
      "26%\n",
      "27%\n",
      "27%\n",
      "27%\n",
      "27%\n",
      "27%\n",
      "28%\n",
      "28%\n",
      "28%\n",
      "28%\n",
      "28%\n",
      "28%\n",
      "29%\n",
      "29%\n",
      "29%\n",
      "29%\n",
      "29%\n",
      "29%\n",
      "30%\n",
      "30%\n",
      "30%\n",
      "30%\n",
      "30%\n",
      "30%\n",
      "31%\n",
      "31%\n",
      "31%\n",
      "31%\n",
      "31%\n",
      "32%\n",
      "32%\n",
      "32%\n",
      "32%\n",
      "32%\n",
      "32%\n",
      "33%\n",
      "33%\n",
      "33%\n",
      "33%\n",
      "33%\n",
      "33%\n",
      "34%\n",
      "34%\n",
      "34%\n",
      "34%\n",
      "34%\n",
      "35%\n",
      "35%\n",
      "35%\n",
      "35%\n",
      "35%\n",
      "35%\n",
      "36%\n",
      "36%\n",
      "36%\n",
      "36%\n",
      "36%\n",
      "36%\n",
      "37%\n",
      "37%\n",
      "37%\n",
      "37%\n",
      "37%\n",
      "38%\n",
      "38%\n",
      "38%\n",
      "38%\n",
      "38%\n",
      "38%\n",
      "39%\n",
      "39%\n",
      "39%\n",
      "39%\n",
      "39%\n",
      "39%\n",
      "40%\n",
      "40%\n",
      "40%\n",
      "40%\n",
      "40%\n",
      "40%\n",
      "41%\n",
      "41%\n",
      "41%\n",
      "41%\n",
      "41%\n",
      "42%\n",
      "42%\n",
      "42%\n",
      "42%\n",
      "42%\n",
      "42%\n",
      "43%\n",
      "43%\n",
      "43%\n",
      "43%\n",
      "43%\n",
      "43%\n",
      "44%\n",
      "44%\n",
      "44%\n",
      "44%\n",
      "44%\n",
      "45%\n",
      "45%\n",
      "45%\n",
      "45%\n",
      "45%\n",
      "45%\n",
      "46%\n",
      "46%\n",
      "46%\n",
      "46%\n",
      "46%\n",
      "46%\n",
      "47%\n",
      "47%\n",
      "47%\n",
      "47%\n",
      "47%\n",
      "47%\n",
      "48%\n",
      "48%\n",
      "48%\n",
      "48%\n",
      "48%\n",
      "49%\n",
      "49%\n",
      "49%\n",
      "49%\n",
      "49%\n",
      "49%\n",
      "50%\n",
      "50%\n",
      "50%\n",
      "50%\n",
      "50%\n",
      "50%\n",
      "51%\n",
      "51%\n",
      "51%\n",
      "51%\n",
      "51%\n",
      "52%\n",
      "52%\n",
      "52%\n",
      "52%\n",
      "52%\n",
      "52%\n",
      "53%\n",
      "53%\n",
      "53%\n",
      "53%\n",
      "53%\n",
      "53%\n",
      "54%\n",
      "54%\n",
      "54%\n",
      "54%\n",
      "54%\n",
      "54%\n",
      "55%\n",
      "55%\n",
      "55%\n",
      "55%\n",
      "55%\n",
      "56%\n",
      "56%\n",
      "56%\n",
      "56%\n",
      "56%\n",
      "56%\n",
      "57%\n",
      "57%\n",
      "57%\n",
      "57%\n",
      "57%\n",
      "57%\n",
      "58%\n",
      "58%\n",
      "58%\n",
      "58%\n",
      "58%\n",
      "59%\n",
      "59%\n",
      "59%\n",
      "59%\n",
      "59%\n",
      "59%\n",
      "60%\n",
      "60%\n",
      "60%\n",
      "60%\n",
      "60%\n",
      "60%\n",
      "61%\n",
      "61%\n",
      "61%\n",
      "61%\n",
      "61%\n",
      "61%\n",
      "62%\n",
      "62%\n",
      "62%\n",
      "62%\n",
      "62%\n",
      "63%\n",
      "63%\n",
      "63%\n",
      "63%\n",
      "63%\n",
      "63%\n",
      "64%\n",
      "64%\n",
      "64%\n",
      "64%\n",
      "64%\n",
      "64%\n",
      "65%\n",
      "65%\n",
      "65%\n",
      "65%\n",
      "65%\n",
      "66%\n",
      "66%\n",
      "66%\n",
      "66%\n",
      "66%\n",
      "66%\n",
      "67%\n",
      "67%\n",
      "67%\n",
      "67%\n",
      "67%\n",
      "67%\n",
      "68%\n",
      "68%\n",
      "68%\n",
      "68%\n",
      "68%\n",
      "68%\n",
      "69%\n",
      "69%\n",
      "69%\n",
      "69%\n",
      "69%\n",
      "70%\n",
      "70%\n",
      "70%\n",
      "70%\n",
      "70%\n",
      "70%\n",
      "71%\n",
      "71%\n",
      "71%\n",
      "71%\n",
      "71%\n",
      "71%\n",
      "72%\n",
      "72%\n",
      "72%\n",
      "72%\n",
      "72%\n",
      "73%\n",
      "73%\n",
      "73%\n",
      "73%\n",
      "73%\n",
      "73%\n",
      "74%\n",
      "74%\n",
      "74%\n",
      "74%\n",
      "74%\n",
      "74%\n",
      "75%\n",
      "75%\n",
      "75%\n",
      "75%\n",
      "75%\n",
      "76%\n",
      "76%\n",
      "76%\n",
      "76%\n",
      "76%\n",
      "76%\n",
      "77%\n",
      "77%\n",
      "77%\n",
      "77%\n",
      "77%\n",
      "77%\n",
      "78%\n",
      "78%\n",
      "78%\n",
      "78%\n",
      "78%\n",
      "78%\n",
      "79%\n",
      "79%\n",
      "79%\n",
      "79%\n",
      "79%\n",
      "80%\n",
      "80%\n",
      "80%\n",
      "80%\n",
      "80%\n",
      "80%\n",
      "81%\n",
      "81%\n",
      "81%\n",
      "81%\n",
      "81%\n",
      "81%\n",
      "82%\n",
      "82%\n",
      "82%\n",
      "82%\n",
      "82%\n",
      "83%\n",
      "83%\n",
      "83%\n",
      "83%\n",
      "83%\n",
      "83%\n",
      "84%\n",
      "84%\n",
      "84%\n",
      "84%\n",
      "84%\n",
      "84%\n",
      "85%\n",
      "85%\n",
      "85%\n",
      "85%\n",
      "85%\n",
      "85%\n",
      "86%\n",
      "86%\n",
      "86%\n",
      "86%\n",
      "86%\n",
      "87%\n",
      "87%\n",
      "87%\n",
      "87%\n",
      "87%\n",
      "87%\n",
      "88%\n",
      "88%\n",
      "88%\n",
      "88%\n",
      "88%\n",
      "88%\n",
      "89%\n",
      "89%\n",
      "89%\n",
      "89%\n",
      "89%\n",
      "90%\n",
      "90%\n",
      "90%\n",
      "90%\n",
      "90%\n",
      "90%\n",
      "91%\n",
      "91%\n",
      "91%\n",
      "91%\n",
      "91%\n",
      "91%\n",
      "92%\n",
      "92%\n",
      "92%\n",
      "92%\n",
      "92%\n",
      "92%\n",
      "93%\n",
      "93%\n",
      "93%\n",
      "93%\n",
      "93%\n",
      "94%\n",
      "94%\n",
      "94%\n",
      "94%\n",
      "94%\n",
      "94%\n",
      "95%\n",
      "95%\n",
      "95%\n",
      "95%\n",
      "95%\n",
      "95%\n",
      "96%\n",
      "96%\n",
      "96%\n",
      "96%\n",
      "96%\n",
      "97%\n",
      "97%\n",
      "97%\n",
      "97%\n",
      "97%\n",
      "97%\n",
      "98%\n",
      "98%\n",
      "98%\n",
      "98%\n",
      "98%\n",
      "98%\n",
      "99%\n",
      "99%\n",
      "99%\n",
      "99%\n",
      "99%\n",
      "99%\n",
      "{'id': '2583411', 'label': '0', 'features': [(3958, 2), (17904, 6), (12452, 6), (2635, 2), (14769, 28), (1363, 2), (7549, 2), (4638, 3), (19804, 2), (6586, 4), (14769, 1256), (6586, 1254), (23761, 9), (805, 23), (9795, 3), (5908, 2), (6849, 6), (13284, 3), (23423, 3), (3491, 6), (18328, 3), (7335, 5), (17795, 2), (3731, 2), (306, 14), (18124, 27), (22516, 16), (10550, 3), (8991, 12), (1488, 3), (11358, 3), (18199, 7), (1565, 12), (7688, 3), (16687, 3), (23483, 3), (18473, 3), (6793, 3), (23670, 1264), (14608, 2), (23670, 2), (22822, 2), (9170, 9), (8368, 6), (23606, 3), (14185, 2), (9082, 3), (7466, 7), (20522, 6), (8338, 2), (5093, 3), (195, 3), (13768, 3), (6316, 6), (11210, 25), (508, 3), (17328, 2), (24285, 40), (2143, 9), (24285, 6), (17819, 3), (20082, 2), (188, 2), (5254, 136), (13240, 24), (17441, 16), (18439, 2), (23575, 6), (24544, 3), (729, 3), (6522, 2), (19271, 3), (24567, 4), (6774, 12), (22590, 3), (17965, 3), (11153, 3), (19414, 14), (24549, 969), (941, 2), (7301, 2), (24524, 2), (8623, 2), (9349, 2), (9349, 10), (17832, 571), (9876, 2), (16646, 2), (13393, 9), (22446, 7), (17345, 37), (961, 7), (961, 9), (12002, 16), (9498, 2), (9373, 2), (398, 16), (10150, 16), (12996, 1254), (4420, 10), (15130, 1), (15130, 3), (22347, 19), (15851, 53), (15635, 7), (24473, 8), (20024, 564), (7212, 10), (22687, 2), (7949, 16), (3660, 564), (4531, 2), (7206, 609), (12537, 3), (1853, 128), (23452, 14), (7998, 2), (7336, 16), (2781, 1258), (19870, 2), (8039, 18), (8347, 7), (24097, 15), (24097, 3), (9371, 3), (16662, 25), (12936, 4), (16662, 24), (18291, 6), (14393, 3), (14909, 22), (6697, 7), (22939, 6), (1865, 3), (16338, 8), (6330, 5), (18041, 5), (17057, 10), (12302, 51), (18016, 564), (9918, 6), (20716, 16), (5352, 15), (16031, 3), (3124, 3), (1683, 3), (2813, 6), (5352, 18), (16031, 3), (2813, 3), (15253, 3), (18004, 3), (4179, 4), (18065, 2), (18647, 7), (23967, 12), (23967, 7), (17467, 3), (17701, 3), (677, 8), (5949, 2), (8849, 2), (8014, 6), (8014, 3), (10510, 3), (10510, 8), (13460, 12), (3685, 3), (252, 5), (17867, 10), (15285, 40), (19165, 2), (17900, 1255), (17900, 2), (17900, 2), (16778, 2), (11439, 2), (15556, 2), (3231, 7), (17263, 5), (3589, 3), (16426, 571), (4899, 574), (16417, 2), (24247, 7), (15436, 2), (15436, 7), (6844, 2), (6132, 7), (14226, 13), (20778, 2), (6962, 2), (13846, 16), (14885, 5), (7807, 3), (12535, 3), (12535, 6), (13630, 7), (2009, 2), (10086, 2), (20452, 1254), (21751, 3), (5996, 3), (21751, 8), (2204, 2), (23449, 16), (10380, 2), (9293, 2), (11837, 10), (15026, 24), (18544, 6), (17532, 2), (14396, 16), (19151, 7), (22497, 142), (17662, 136), (20935, 1), (20935, 3), (1636, 30), (7059, 26), (2360, 1256), (5006, 1256), (2360, 35), (23071, 16), (15908, 16), (5518, 7), (4426, 566), (24402, 2), (13529, 29), (17816, 3), (10977, 2), (19214, 7), (12154, 7), (9153, 4), (7980, 5), (23109, 3), (6252, 41), (275, 77), (275, 1286), (8837, 2), (23982, 564), (6252, 24), (18091, 24), (1219, 564), (19236, 3), (18465, 3), (4869, 9), (4869, 564), (7506, 12), (11196, 2), (16881, 2), (9279, 3), (9279, 24), (20644, 3), (12589, 10), (23812, 3), (14061, 3), (4483, 3), (21235, 8), (11261, 3), (12933, 3), (21429, 7), (18385, 18), (12711, 3), (15640, 3), (5534, 6), (5534, 3), (400, 3), (15283, 4), (7251, 3), (5941, 8), (13243, 2), (18633, 42), (9643, 16), (12418, 4), (6835, 12), (6835, 30), (8221, 2), (23808, 16), (150, 16), (14503, 3), (2607, 12), (5231, 16), (14245, 2), (11517, 3), (12952, 2), (6451, 3), (4946, 24), (18923, 12), (18923, 6), (22230, 6), (9917, 34), (14854, 9), (6398, 3), (21651, 3), (14854, 3), (10749, 16), (4801, 2), (16549, 2), (24364, 3), (23655, 3), (5009, 2), (5009, 2), (15676, 1), (19376, 7), (3838, 9), (10018, 24), (10018, 9), (21849, 3), (24640, 2), (6223, 1), (15497, 6), (24609, 3), (6829, 40), (4466, 3), (16865, 10), (20172, 12), (22744, 3), (4678, 30), (4300, 6), (11825, 7), (16256, 21), (14023, 2), (20687, 2), (19116, 8), (949, 2), (13330, 6), (19985, 2), (2807, 7), (566, 7), (6182, 8), (11748, 2), (22319, 4), (9126, 6), (1097, 3), (10138, 3), (20412, 8), (12048, 2), (24376, 2), (89, 2), (16006, 1), (8037, 7), (11577, 16), (6406, 7), (8148, 3), (22486, 13), (15742, 2), (8313, 9), (14247, 1070), (16579, 100), (6291, 2), (1582, 2), (3342, 130), (19360, 19), (23600, 7), (14304, 4), (24119, 16), (10041, 16), (7242, 4), (17534, 3), (3263, 3), (17534, 8), (20328, 2), (11449, 2), (9866, 3), (14496, 2), (16766, 570), (16316, 7), (21986, 7), (1516, 7), (23573, 7), (22312, 5), (13345, 13), (11901, 12), (2169, 2), (5782, 18), (19357, 7), (12685, 7), (24015, 8), (24015, 16), (5342, 14), (5051, 12), (5342, 3), (17097, 3), (8546, 16), (23465, 24), (4408, 24), (15955, 3), (13343, 3), (18210, 38), (17479, 7), (17479, 16), (22849, 8), (21792, 2), (2358, 8), (15327, 10), (17874, 6), (9401, 8), (15277, 8), (233, 20), (5890, 3), (19597, 3), (21660, 7), (2080, 16), (9472, 16), (22471, 19), (13573, 13), (22977, 33), (7266, 3), (2343, 12), (21212, 3), (15340, 3), (17617, 7), (6067, 6), (22619, 3), (23882, 2), (12865, 2), (2467, 1072), (16258, 2), (20893, 8), (6747, 16), (18298, 8), (16673, 27), (20011, 3), (10351, 906), (14095, 6), (2568, 3), (14095, 3), (22421, 3), (14095, 15), (13415, 2), (8954, 3), (4275, 2), (10690, 12), (19180, 564), (13014, 564), (19180, 2), (3233, 3), (16027, 3), (11547, 8), (11672, 206), (5863, 9), (14422, 3), (2425, 6), (2425, 9), (9777, 2), (8588, 2), (16959, 10), (11581, 18), (1671, 3), (11019, 3), (6266, 3), (19471, 2), (14106, 3), (8742, 3), (18688, 3), (2971, 2), (2971, 8), (10450, 2), (9319, 3), (6504, 3), (23426, 3), (64, 2), (11694, 6), (19809, 6), (21772, 6), (10680, 9), (11787, 2), (8847, 2), (17556, 2), (14807, 2), (18832, 3), (7746, 2), (7746, 9), (16796, 19), (13553, 2), (13553, 12), (83, 3), (10411, 12), (19727, 10), (8818, 9), (13251, 2), (1250, 12), (14105, 2), (19606, 2), (5540, 61), (7578, 2), (1961, 2), (4565, 2), (4565, 564), (4565, 2), (12491, 6), (2184, 3), (2184, 3), (2913, 2), (19936, 7), (1988, 41), (12518, 3), (8698, 3), (1988, 6), (23330, 7), (14350, 7), (12152, 7), (9487, 2), (4937, 12), (21239, 1360), (21239, 564), (21239, 9), (21239, 6), (22701, 6), (15393, 3), (1989, 3), (11440, 2), (8226, 9), (5389, 3), (24198, 3), (20579, 2), (18671, 2), (10084, 1254), (6916, 4), (19965, 3), (16349, 3), (24565, 12), (1779, 10), (723, 3), (7419, 119), (7783, 79), (17968, 10), (8382, 3), (1881, 3), (17968, 2), (14647, 2), (20298, 3), (15250, 24), (7643, 14), (19513, 10), (6690, 18), (1517, 2), (16427, 6), (9975, 6), (11601, 564), (10219, 24), (10704, 2), (8240, 7), (15705, 2), (2427, 2), (7898, 12), (10043, 16), (19220, 16), (5767, 564), (24034, 3), (24034, 4), (10950, 2), (10043, 36), (3065, 3), (11347, 4), (9618, 24), (20563, 3), (10043, 14), (20489, 2), (11600, 3), (24215, 3), (9206, 3), (8970, 7), (17529, 3), (23347, 17), (16345, 7), (6930, 7), (12313, 3), (17806, 16), (17806, 7), (23149, 3), (1200, 14), (1200, 9), (22388, 3), (18360, 3), (22388, 2), (23973, 2), (6549, 564), (11761, 5), (6577, 6), (15736, 3), (13832, 1254), (19369, 3), (16370, 3), (7210, 2), (20583, 2), (7210, 2), (1792, 7), (9644, 14), (17128, 2), (3887, 2), (11160, 2), (8281, 30), (1898, 4), (1898, 2), (10720, 8), (23669, 708), (18436, 236), (20259, 246), (220, 246), (23669, 24), (4791, 2508), (17208, 1254), (17208, 1254), (9783, 24), (13435, 58), (12230, 7), (4293, 33), (10163, 2), (17445, 38), (11611, 16), (19895, 22), (17445, 6), (17445, 8), (20972, 8), (18105, 2), (8040, 7), (8040, 3), (21727, 3), (15983, 56), (3570, 9), (3570, 1254), (13515, 6), (15867, 5), (18448, 3), (18448, 14), (2104, 1254), (20741, 2), (20741, 5), (19882, 12), (2338, 5), (19882, 4), (21370, 2), (23566, 124), (8589, 3), (23566, 1266), (3459, 7), (23566, 355), (2241, 20), (4302, 8), (4302, 2), (18102, 2), (23162, 16), (8651, 3), (16511, 3), (9021, 7), (17279, 7), (16571, 9), (16571, 5), (14723, 3), (716, 3), (11736, 3), (6055, 3), (17234, 20), (17234, 24), (16484, 8), (14382, 5), (3761, 2), (23317, 15), (4223, 1258), (4092, 1254), (18437, 2), (973, 564), (3412, 564), (3412, 3), (1711, 12), (20207, 3), (20400, 1070), (5453, 3), (17460, 9), (16458, 20), (16458, 5), (1188, 29), (11935, 3), (10067, 9), (23656, 2), (22900, 7), (22900, 3), (1188, 20), (10067, 2), (3727, 3), (12464, 2), (18483, 2), (15432, 6), (21384, 6), (2652, 2), (8408, 60), (16396, 8), (8408, 272), (8564, 9), (2265, 5), (15282, 3), (12035, 3), (1230, 6), (1230, 9), (16145, 8), (11909, 19), (6168, 3), (15123, 6), (752, 3), (17073, 3), (11715, 4), (13931, 4), (13511, 564), (3884, 16), (13398, 1), (18881, 3), (9209, 3), (1099, 6), (11357, 3), (18034, 2), (18720, 4), (802, 8), (16801, 4), (14776, 8), (10891, 3), (7587, 3), (23129, 2), (9911, 3), (6277, 3), (1028, 19), (17400, 12), (1028, 3), (17442, 32), (11152, 17), (12168, 1257), (13572, 3), (18909, 17), (1710, 4), (20048, 5), (21339, 21), (14703, 12), (10069, 12), (21243, 2), (6612, 3), (10400, 3), (22029, 3), (15339, 3), (20980, 3), (16062, 3), (4529, 28), (23886, 3), (4597, 6), (2157, 3), (4089, 6), (22480, 3), (628, 3), (4529, 27), (10601, 4), (17364, 2), (12826, 3), (2620, 3), (13770, 2), (4170, 11), (9393, 7), (9177, 2), (4170, 30), (12709, 3), (17253, 3), (4529, 2), (21737, 2), (5100, 9), (5842, 46), (5842, 7), (5842, 3), (8147, 2), (8147, 10), (17053, 3), (4769, 7), (21895, 16), (21895, 16), (11791, 23), (2943, 4), (21780, 3), (3048, 2), (3118, 22), (5229, 6), (10082, 16), (4276, 16), (20692, 3), (22352, 2), (1507, 12), (24309, 10), (18753, 3), (11782, 3), (22450, 571), (24302, 18), (2171, 3), (5023, 3), (7543, 17), (13436, 3), (10003, 46), (15868, 10), (15868, 10), (18764, 15), (13852, 2), (18764, 6), (10488, 7), (9756, 6), (10622, 1), (9971, 5), (9971, 2), (5601, 3), (19217, 2), (19324, 8), (6821, 3), (20191, 7), (21665, 12), (17068, 564), (878, 1), (23127, 2), (16722, 7), (18756, 2), (4439, 20), (1768, 3), (8012, 3), (21102, 3), (8361, 3), (14127, 3), (4439, 2), (4439, 3), (66, 3), (462, 3), (23354, 3), (462, 18), (13952, 3), (9834, 3), (16747, 3), (18073, 1254), (13835, 4), (24410, 2), (18864, 6), (1991, 2), (12566, 9), (20810, 3), (12566, 3), (20829, 3), (14297, 4), (14297, 1070), (22052, 2), (19338, 8), (4132, 2), (19338, 12), (22003, 4), (21927, 16), (10815, 3), (22667, 3), (10815, 18), (12071, 3), (9581, 3), (6623, 3), (5076, 3), (19364, 16), (21724, 8), (3975, 8), (9406, 16), (5496, 16), (14419, 24), (3870, 1), (4165, 5), (8539, 2), (21364, 9), (8990, 19), (14414, 8), (8990, 9), (8410, 2), (15111, 1273), (8207, 10), (3001, 8), (15111, 4), (16612, 3), (22841, 1273), (16682, 2), (16682, 6), (14187, 2), (9330, 2), (8969, 7), (22841, 50), (24018, 3), (818, 3), (1024, 6), (4373, 3), (5056, 6), (23762, 3), (10799, 2), (15281, 3), (995, 3), (14997, 3), (6460, 2), (3369, 3), (16738, 2), (21065, 3), (21865, 3), (17957, 43), (9308, 33), (17957, 3), (17957, 16), (3733, 16), (16751, 16), (6179, 8), (21476, 6), (14748, 8), (14748, 1254), (3432, 10), (8209, 3), (22828, 5), (17702, 7), (22615, 2), (24373, 16), (15158, 6), (18474, 6), (5151, 2), (20071, 2), (12937, 2), (14195, 7), (12240, 7), (21248, 10), (20046, 12), (14843, 20), (5222, 7), (8393, 24), (13562, 12), (16528, 7), (4338, 9), (4338, 2), (8501, 5), (22833, 4), (21311, 6), (5661, 19), (16102, 3), (21326, 3), (19555, 564), (21869, 564), (11013, 647), (3598, 1259), (698, 2), (5696, 3), (12606, 10), (9024, 6), (14772, 3), (7850, 16), (22884, 16), (20394, 7), (21733, 3), (8309, 2), (22147, 3), (11289, 3), (2209, 5), (17046, 2), (21693, 6), (19056, 2), (4530, 5), (1506, 2), (4651, 3), (10133, 236), (1191, 12), (1191, 7), (20999, 36), (2431, 26), (2431, 10), (8974, 6), (23223, 55), (16640, 3), (6824, 8), (7416, 3), (19708, 3), (23223, 7), (7708, 38), (16089, 570), (19741, 15), (21190, 10), (21190, 8), (9009, 8), (15623, 564), (18946, 16), (9166, 40), (20141, 10), (11462, 12), (5552, 88), (465, 2), (5552, 14), (1010, 3), (21155, 3), (19325, 24), (11206, 5), (22306, 5), (7626, 7), (2945, 16), (10406, 4), (2507, 8), (2507, 15), (19344, 7), (11026, 22), (18757, 16), (411, 3), (9750, 4), (12388, 2), (22957, 11), (4476, 6), (12520, 2), (7603, 3), (10775, 3), (19567, 6), (2186, 3), (1708, 1082), (16137, 12), (1708, 16), (17049, 7), (8091, 596), (12204, 16), (24259, 2), (14168, 3), (7591, 4), (560, 4), (8091, 16), (9626, 2), (21415, 12), (20733, 12), (4503, 7), (561, 2), (9077, 8), (21523, 35), (14077, 3), (1464, 236), (1464, 236), (10844, 3350), (19128, 2), (18248, 571), (10844, 3812), (16617, 2), (21209, 3), (21209, 3), (16968, 3), (18446, 5), (11438, 2), (13963, 3), (18446, 3), (12060, 12), (584, 2), (20950, 2), (10947, 4), (22325, 574), (1442, 9), (22436, 2522), (17869, 1254), (9158, 2), (20881, 2), (21948, 1254), (13380, 2), (22436, 12), (7121, 3), (16262, 3), (21470, 3), (19318, 16), (10783, 7), (22473, 14), (3850, 12), (22903, 66), (12570, 3), (14710, 7), (22128, 24), (12754, 3), (9469, 3), (7294, 6), (7294, 7), (14344, 7), (6170, 2), (3622, 16), (23761, 2), (22167, 2), (24500, 2), (13222, 3), (13054, 2), (17896, 1), (21788, 3), (17345, 4), (11448, 1), (11731, 16), (17901, 2), (12002, 2), (15851, 3), (7212, 10), (7206, 209), (4214, 2), (1261, 2), (8432, 2), (15278, 3), (10196, 2), (18532, 2), (3908, 2), (252, 5), (5079, 2), (20866, 4), (20866, 1), (17532, 1), (5337, 2), (6252, 16), (13986, 2), (15958, 2), (263, 2), (10195, 1), (1382, 5), (14246, 2), (4946, 4), (1296, 2), (24381, 2), (16447, 11), (15972, 2), (13785, 2), (14645, 2), (11901, 3), (16074, 3), (15327, 1), (7686, 11), (4768, 2), (2343, 1), (22728, 7), (16769, 2), (17512, 2), (17114, 11), (16765, 2), (7534, 3), (21239, 9), (12269, 3), (15739, 199), (7643, 5), (19513, 5), (11601, 50), (7898, 2), (18689, 2), (20205, 3), (13779, 3), (19241, 2), (10545, 3), (16985, 2), (15983, 4), (17392, 7), (22928, 2), (18052, 2), (2829, 2), (1349, 2), (5594, 1), (21068, 1), (2625, 4), (11656, 1), (13835, 2), (10815, 1), (6244, 3), (22726, 2), (3271, 2), (1570, 2), (15111, 1), (6179, 2), (22615, 2), (2650, 51), (15852, 50), (23468, 2), (15433, 2), (23223, 1), (17984, 5), (20141, 2), (2507, 4), (2507, 4), (8091, 110), (12204, 110), (7591, 5), (10233, 1), (6602, 1), (9526, 2), (9172, 2), (7528, 2)], '_id': ObjectId('6194c7e220dec0a7312b2092')}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_vocabulary(out_file='vocabulary_100000hq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting MongoDB database\n",
    "#### in container: \n",
    "```bash\n",
    "mongodump --db data -o ./[DUMP_DIR_NAME]\n",
    "```\n",
    "#### in host: \n",
    "```\n",
    "docker cp [CONTAINER_ID]:[DUMP_DIR_NAME] .  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6368e5c98fb2e48815a525cb72517b977ef92526673cca52aa191144e8421d6f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('mlexps': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
